{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429f1b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://towardsdatascience.com/how-to-build-a-wordpiece-tokenizer-for-bert-f505d97dddbb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43faccfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentencepiece # datasets transformers==4.11.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68800ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import *\n",
    "from transformers import *\n",
    "from tokenizers import *\n",
    "# import os\n",
    "# import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c378de",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\n",
    "  \"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"<S>\", \"<T>\"\n",
    "]\n",
    "# if you want to train the tokenizer on both sets\n",
    "# files = [\"train.txt\", \"test.txt\"]\n",
    "# training the tokenizer on the training set\n",
    "files = [\"train.txt\"]\n",
    "# 30,522 vocab is BERT's default vocab size, feel free to tweak\n",
    "vocab_size = 30_522\n",
    "# maximum sequence length, lowering will result to faster training (when increasing batch size)\n",
    "max_length = 512\n",
    "# whether to truncate\n",
    "truncate_longer_samples = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee64bd45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5475fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9cd0b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8380b03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = ['/home/info/MyNotebooks/Datasets/MPT/MPTD/Dataset_MPT/DenunBert.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31bbceb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba00752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TREINANDO O TOKENIZADOR\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "\n",
    "\n",
    "# initialize\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    clean_text=True,\n",
    "    handle_chinese_chars=False,\n",
    "    strip_accents=False,\n",
    "    lowercase=False\n",
    ")\n",
    "# and train\n",
    "tokenizer.train(files=paths, vocab_size=30_000, min_frequency=2,\n",
    "                limit_alphabet=1000, wordpieces_prefix='##',\n",
    "                special_tokens=['[PAD', '[UNK]', '[CLS]', '[SEP]', '[MASK]'])\n",
    "\n",
    "tokenizer.enable_truncation(max_length=max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ced819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SALVANDO O TOKENIZADOR - Arquivo .json\n",
    "tokenizer.save(\"/home/info/MyNotebooks/DenunBert/Tokenizer/BertWordPiece/BertWordPiece.json\") # Cria BertWordPiece.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7ad7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SALVANDO O TOKENIZADOR - vocab.json\n",
    "# Cria vocab.txt\n",
    "# During tokenization vocab.txt is used to map text to tokens, which are then mapped to token IDs based on the row \n",
    "# number of the token in vocab.txt — those IDs are then fed into BERT!\n",
    "\n",
    "tokenizer.save_model(\"/home/info/MyNotebooks/DenunBert/Tokenizer/BertWordPiece/\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b064a77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3941ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58418577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231b9f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARREGANDO O TOKENIZADOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e1b947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "WordPieceTokenizer = BertTokenizer.from_pretrained(\"/home/info/MyNotebooks/DenunBert/Tokenizer/BertWordPiece/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c2e37b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaa4c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "WordPieceTokenizer(\"O Ministério Público do Trabalho resgatou milhares de pessoas\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa101ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As our vocab.txt file contains the mappings for our tokens and token IDs (e.g., the row numbers) — we can access \n",
    "# the tokens by aligning our input_ids token IDs to the rows in vocab.txt:\n",
    "\n",
    "with open(\"/home/info/MyNotebooks/DenunBert/Tokenizer/BertWordPiece/vocab.txt\", 'r') as fp:\n",
    "    vocab = fp.read().split('\\n')\n",
    "\n",
    "vocab[2],vocab[81],vocab[7955],vocab[5098],vocab[254],vocab[323],vocab[14577],vocab[1041],vocab[14781],vocab[219],vocab[900],vocab[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c03bbea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e2aee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fdfa53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca92fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING DATAFRAMES DATASETDICTS FOR TRAINING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7692f762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import pandas as pd\n",
    "\n",
    "paths = ['/home/info/MyNotebooks/Datasets/MPT/MPTD/Dataset_MPT/DenunBert.txt']\n",
    "\n",
    "\n",
    "ds_dir = \"/home/info/MyNotebooks/Datasets/MPT/MPTD/\"\n",
    "#ds_dir = \"/home/info/.cache/huggingface/datasets/\"\n",
    "ds = datasets.load_from_disk(ds_dir+\"Dataset_MPT\")\n",
    "\n",
    "train_dataset = ds[\"train\"][\"tip_text\"] # Retorna a coluna tip_text como objeto list!\n",
    "test_dataset  = ds[\"validation\"][\"tip_text\"]\n",
    "\n",
    "train_df = pd.DataFrame(train_dataset, columns=['text']) # renomeia a coluna para 'text'\n",
    "test_df  = pd.DataFrame(test_dataset, columns=['text'])\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df) # Instancia os Datasetdicts\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "\n",
    "train_dir     = \"/home/info/MyNotebooks/Datasets/MPT/MPTD/Partials/Bert/train_dataset\" \n",
    "test_dir      = \"/home/info/MyNotebooks/Datasets/MPT/MPTD/Partials/Bert/test_dataset\"\n",
    "\n",
    "train_dataset.save_to_disk(train_dir)\n",
    "test_dataset.save_to_disk(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a568d439",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06869196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the Dataset\n",
    "# Now that we have the tokenizer ready, the below code is responsible for tokenizing the dataset:\n",
    "\n",
    "def encode_with_truncation(examples):\n",
    "  \"\"\"Mapping function to tokenize the sentences passed with truncation\"\"\"\n",
    "  return WordPieceTokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=max_length, return_special_tokens_mask=True)\n",
    "\n",
    "def encode_without_truncation(examples):\n",
    "  \"\"\"Mapping function to tokenize the sentences passed without truncation\"\"\"\n",
    "  return WordPieceTokenizer(examples[\"text\"], return_special_tokens_mask=True)\n",
    "\n",
    "# the encode function will depend on the truncate_longer_samples variable\n",
    "encode = encode_with_truncation if truncate_longer_samples else encode_without_truncation\n",
    "\n",
    "# tokenizing the train dataset\n",
    "train_tokenized_dataset = train_dataset.map(encode, batched=True)\n",
    "# tokenizing the testing dataset\n",
    "test_tokenized_dataset = test_dataset.map(encode, batched=True)\n",
    "\n",
    "if truncate_longer_samples:\n",
    "  # remove other columns and set input_ids and attention_mask as \n",
    "  train_tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "  test_tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "else:\n",
    "  test_tokenized_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])\n",
    "  train_tokenized_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])\n",
    "    \n",
    "train_tokenized_dataset, test_tokenized_dataset\n",
    "\n",
    "# Salvar os datasets tokenizados para nao necessitar tokeniza-los novamente caso o notebook reinicie o kernel.\n",
    "\n",
    "train_tkz_dir = \"/home/info/MyNotebooks/Datasets/MPT/MPTD/Partials/Bert/train_tokenized_dataset\"\n",
    "test_tkz_dir  = \"/home/info/MyNotebooks/Datasets/MPT/MPTD/Partials/Bert/test_tokenized_dataset\"\n",
    "\n",
    "train_tokenized_dataset.save_to_disk(train_tkz_dir)\n",
    "test_tokenized_dataset.save_to_disk(test_tkz_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d4635a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac0e981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, in the case of setting truncate_longer_samples to False, we need to join our untruncated samples together and cut them into fixed-size vectors since the model expects a fixed-sized sequence during training:\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of\n",
    "# max_seq_length.\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= max_length:\n",
    "        total_length = (total_length // max_length) * max_length\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + max_length] for i in range(0, total_length, max_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result\n",
    "# Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a\n",
    "# remainder for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value\n",
    "# might be slower to preprocess.\n",
    "#\n",
    "# To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n",
    "# https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map\n",
    "\n",
    "if not truncate_longer_samples:\n",
    "  train_dataset = train_dataset.map(group_texts, batched=True, batch_size=2_000,\n",
    "                                    desc=f\"Grouping texts in chunks of {max_length}\")\n",
    "  test_dataset = test_dataset.map(group_texts, batched=True, batch_size=2_000,\n",
    "                                  num_proc=4, desc=f\"Grouping texts in chunks of {max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9080dd31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995419cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3e635e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af0c61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATACOLLATOR\n",
    "#\n",
    "# initialize the data collator, randomly masking 20% (default is 15%) of the tokens for the Masked Language\n",
    "# Modeling (MLM) task\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=WordPieceTokenizer, mlm=True, mlm_probability=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e171b732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83284367",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d761620b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ef6041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have huge custom dataset separated into files\n",
    "# load the splitted files\n",
    "\n",
    "files = ['/home/info/MyNotebooks/Datasets/MPT/MPTD/Dataset_MPT/DenunBert.txt']\n",
    "# dataset = load_dataset(\"text\", data_files=files, split=\"train\")\n",
    "dataset = load_dataset(\"text\", data_files=files)\n",
    "dataset\n",
    "#dataset_temp = dataset['train']\n",
    "# dataset[\"train\"][\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13bb2d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d65ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e9c303",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0004c4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURING THE MODEL\n",
    "#\n",
    "# initialize the model with the config\n",
    "from transformers import BertConfig, BertForMaskedLM\n",
    "\n",
    "# 30,522 vocab is BERT's default vocab size, feel free to tweak\n",
    "vocab_size = 30_522\n",
    "# maximum sequence length, lowering will result to faster training (when increasing batch size)\n",
    "max_length = 512\n",
    "# whether to truncate\n",
    "truncate_longer_samples = True\n",
    "\n",
    "model_config = BertConfig(vocab_size=vocab_size, max_position_embeddings=max_length)\n",
    "\n",
    "model = BertForMaskedLM(config=model_config)\n",
    "\n",
    "# We initialize the model config using BertConfig, and pass the vocabulary size as well as the maximum sequence \n",
    "# length. We then pass the config to BertForMaskedLM to initialize the model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e3fa83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec1fba0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02093152",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8985a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se o KERNEL DO NOTEBOOK REINICIAR, alem dos passos que reconstruem o modelo e recarregam o tokenizador, devemos\n",
    "# recarregar também os datasets tokenizados (ou os datasets originais):\n",
    "import datasets\n",
    "\n",
    "ds_train_dir = \"/home/info/MyNotebooks/Datasets/MPT/MPTD/Partials/Bert/train_tokenized_dataset\"\n",
    "ds_test_dir  = \"/home/info/MyNotebooks/Datasets/MPT/MPTD/Partials/Bert/test_tokenized_dataset\"\n",
    "\n",
    "train_tokenized_dataset = datasets.load_from_disk(ds_train_dir)\n",
    "test_tokenized_dataset  = datasets.load_from_disk(ds_test_dir)\n",
    "\n",
    "train_tokenized_dataset, test_tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fc643e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953df281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdf1813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e4e8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING THE MODEL\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "model_path = \"/home/info/MyNotebooks/DenunBert/Tokenizer/BertWordPiece/Model\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_path,          # output directory to where save model checkpoint\n",
    "    evaluation_strategy=\"steps\",    # evaluate each `logging_steps` steps\n",
    "    overwrite_output_dir=True,      \n",
    "    num_train_epochs=1, #10,            # number of training epochs, feel free to tweak\n",
    "    per_device_train_batch_size=10, # the training batch size, put it as high as your GPU memory fits\n",
    "    gradient_accumulation_steps=1, #8,  # accumulating the gradients before updating the weights\n",
    "    per_device_eval_batch_size=64,  # evaluation batch size\n",
    "    logging_steps=500,             # evaluate, log and save model checkpoints every 1000 step\n",
    "    save_steps=500,\n",
    "    # load_best_model_at_end=True,  # whether to load the best model (in terms of loss) at the end of training\n",
    "    # save_total_limit=3,           # whether you don't have much space so you let only 3 model weights saved in the disk\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769bb69c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f719c965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the trainer and pass everything to it\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_tokenized_dataset,\n",
    "    eval_dataset=test_tokenized_dataset,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349b1387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deaeaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0543b3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a339dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b833b631",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aac038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternativamente ao tempo de treinamento de um modelo de linguagem base no dominio, podemos tomar algum outro\n",
    "# generico em portugues (Bertimbau, Dominio Juridico?) como base e usando TransformerAdapter, refina-lo nas tarefas \n",
    "# especificas usando nosso dataset do dominio requerido.\n",
    "# \n",
    "# Ou, se nem assim houver tempo, montar a arquitetura da solução com os modelos transformers disponíveis\n",
    "# E avaliar o resultado de suas aplicações.\n",
    "# https://huggingface.co/neuralmind/bert-large-portuguese-cased\n",
    "#\n",
    "# Podemos tambem usar um AdapterLang em modelo de NLP treinado em dataset juridico em ingles!\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c8c9a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ee1ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45b8df4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05edfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model checkpoint\n",
    "model = BertForMaskedLM.from_pretrained(os.path.join(model_path, \"checkpoint-10000\"))\n",
    "# load the tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7959350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf42473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform predictions\n",
    "examples = [\n",
    "  \"Today's most trending hashtags on [MASK] is Donald Trump\",\n",
    "  \"The [MASK] was cloudy yesterday, but today it's rainy.\",\n",
    "]\n",
    "for example in examples:\n",
    "  for prediction in fill_mask(example):\n",
    "    print(f\"{prediction['sequence']}, confidence: {prediction['score']}\")\n",
    "  print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97831474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aae4a09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc91b78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcd7223",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03ff1d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a308d00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c413b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df40ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b508fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "from transformers import LineByLineTextDataset\n",
    "\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=WordPieceTokenizer,\n",
    "    file_path=\"/home/info/MyNotebooks/Datasets/MPT/MPTD/Dataset_MPT/DenunBert.txt\",\n",
    "    block_size=128,\n",
    ")\n",
    "\n",
    "\n",
    "dataset_temp = dataset['train']\n",
    "d = dataset_temp.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f0cf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import RobertaTokenizer\n",
    "\n",
    "# initialize the tokenizer using the tokenizer we initialized and saved to file\n",
    "#RobToken = RobertaTokenizer.from_pretrained(\"/home/info/MyNotebooks/DenunBert/Tokenizer/BertWordPiece/vocab.txt\", max_len=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5b03ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to train the tokenizer from scratch (especially if you have custom\n",
    "# dataset loaded as datasets object), then run this cell to save it as files\n",
    "# but if you already have your custom data as text files, there is no point using this\n",
    "def dataset_to_text(dataset, output_filename=\"data.txt\"):\n",
    "  \"\"\"Utility function to save dataset text to disk,\n",
    "  useful for using the texts to train the tokenizer \n",
    "  (as the tokenizer accepts files)\"\"\"\n",
    "  with open(output_filename, \"w\") as f:\n",
    "    for t in dataset[\"text\"]:\n",
    "      print(t, file=f)\n",
    "\n",
    "# save the training set to train.txt\n",
    "dataset_to_text(d[\"train\"], \"train.txt\")\n",
    "# save the testing set to test.txt\n",
    "dataset_to_text(d[\"test\"], \"test.txt\")\n",
    "# https://www.thepythoncode.com/article/pretraining-bert-huggingface-transformers-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdbebd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('squad', split='train')\n",
    "\n",
    "dataset.features\n",
    "{'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None),\n",
    "'context': Value(dtype='string', id=None),\n",
    "'id': Value(dtype='string', id=None),\n",
    "'question': Value(dtype='string', id=None),\n",
    "'title': Value(dtype='string', id=None)}\n",
    "# https://huggingface.co/docs/datasets/process"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
